{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a223af9-b4bb-4dea-841f-50077779f119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12b43e7e-fb02-484c-bde3-37cfbbb5b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arxiv_articles_with_html(query, max_results=100):\n",
    "    '''\n",
    "    Return list of ArXiV documents offering an (LaTeXML-postprocessed) HTML variant\n",
    "    '''\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    )\n",
    "    \n",
    "    articles_with_html = []\n",
    "    \n",
    "    for result in client.results(search):\n",
    "        if any(link.title == \"pdf\" for link in result.links):\n",
    "            articles_with_html.append({\n",
    "                'id': result.entry_id,\n",
    "                'title': result.title,\n",
    "                'pdf_url': result.entry_id.replace('abs', 'pdf') + '.pdf',\n",
    "                'html_url': result.entry_id.replace('/abs/', '/html/')\n",
    "            })\n",
    "    \n",
    "    return articles_with_html\n",
    "\n",
    "\n",
    "\n",
    "def download_plain_text_from_html(url):\n",
    "    '''\n",
    "    Download text from URL\n",
    "    '''\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract the text from the HTML\n",
    "    text = soup.get_text()\n",
    "\n",
    "    return text\n",
    "\n",
    "def beautify_text(text):\n",
    "    # Remove leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Normalize multiple spaces and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "    \n",
    "    # Handle special cases for specific artifacts\n",
    "    replacements = {\n",
    "        r'\\\\xa0': ' ',  # Replace non-breaking space with regular space\n",
    "        r'\\\\u2062': '',  # Remove invisible separator\n",
    "        r'\\\\n': '\\n',  # Normalize escaped newlines\n",
    "        r'\\\\t': '\\t',  # Normalize escaped tabs\n",
    "    }\n",
    "    \n",
    "    for pattern, replacement in replacements.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    \n",
    "    # Remove any remaining escape sequences\n",
    "    text = re.sub(r'\\\\', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_title(url):\n",
    "    \"\"\"\n",
    "    Extract title from ArXiV HTML website\n",
    "    \"\"\"\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the title element using its class\n",
    "    title_element = soup.find('h1', class_='ltx_title ltx_title_document')\n",
    "    \n",
    "    # Extract and return the title text\n",
    "    if title_element:\n",
    "        return title_element.get_text(strip=True)\n",
    "    else:\n",
    "        return \"Title not found\"\n",
    "\n",
    "def extract_authors(url):\n",
    "    '''\n",
    "    Extract author information from ArXiV HTML website\n",
    "    '''\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the authors div\n",
    "    authors_div = soup.find('div', class_='ltx_authors')\n",
    "    \n",
    "    # Extract and return the names of the authors\n",
    "    authors = []\n",
    "    if authors_div:\n",
    "        author_spans = authors_div.find_all('span', class_='ltx_personname')\n",
    "        for span in author_spans:\n",
    "            authors.append(span.get_text(strip=True))\n",
    "    \n",
    "    return authors\n",
    "\n",
    "# Function to extract date and domain from the given URL using Selenium\n",
    "def extract_date_and_domain(url, sec_to_timeout:int=3):\n",
    "    # Initialize the WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run headless browser\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Load the page\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the watermark-tr div to be present\n",
    "        watermark_div = WebDriverWait(driver, sec_to_timeout).until(\n",
    "            EC.presence_of_element_located((By.ID, \"watermark-tr\"))\n",
    "        )\n",
    "\n",
    "        # Extract the text from the div\n",
    "        text = watermark_div.text.strip()\n",
    "\n",
    "        # Extract the arXiv ID, domain, and date using regular expression\n",
    "        match = re.search(r'arXiv:\\d{4}\\.\\d{5}(?:v\\d)? \\[(.*?)\\] (\\d{2} \\w{3} \\d{4})', text)\n",
    "        if match:\n",
    "            domain = match.group(1)\n",
    "            date = match.group(2)\n",
    "        else:\n",
    "            domain = ''\n",
    "            date = ''\n",
    "    except TimeoutException:\n",
    "        # Handle timeout exception\n",
    "        domain = ''\n",
    "        date = ''\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "    return {'date': date, 'domain': domain}\n",
    "\n",
    "# Function to extract abstract from the given URL using Selenium\n",
    "def extract_abstract(url, sec_to_timeout=10):\n",
    "    # Initialize the WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run headless browser\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Load the page\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the abstract div to be present\n",
    "        abstract_div = WebDriverWait(driver, sec_to_timeout).until(\n",
    "            EC.presence_of_element_located((By.ID, \"abstract\"))\n",
    "        )\n",
    "\n",
    "        # Extract the text from the div\n",
    "        abstract_text = abstract_div.text.strip()\n",
    "    except TimeoutException:\n",
    "        # Handle timeout exception\n",
    "        abstract_text = None\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "    return abstract_text\n",
    "\n",
    "def extract_author_emails(url, sec_to_timeout=5):\n",
    "    # Initialize the WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run headless browser\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Load the page\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the authors and emails to be present\n",
    "        authors_div = WebDriverWait(driver, sec_to_timeout).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"ltx_authors\"))\n",
    "        )\n",
    "        \n",
    "        # Extract author names\n",
    "        author_elements = authors_div.find_elements(By.CLASS_NAME, \"ltx_personname\")\n",
    "        authors = [author.text.strip() for author in author_elements]\n",
    "\n",
    "        # Extract emails\n",
    "        email_elements = driver.find_elements(By.CLASS_NAME, \"ltx_contact.ltx_role_email\")\n",
    "        emails = [email.text.strip() for email in email_elements]\n",
    "\n",
    "        # Create a dictionary with author names and emails\n",
    "        author_emails = {author: email for author, email in zip(authors, emails)}\n",
    "    except TimeoutException:\n",
    "        # Handle timeout exception\n",
    "        author_emails = {}\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "    return author_emails\n",
    "\n",
    "def post_process_emails(author_emails:dict):\n",
    "    \"\"\"\n",
    "    Post-processes author/email dictinary extracted from ArXiV to match author names and email addresses (if present) \n",
    "    \"\"\"\n",
    "    # Initialize result dictionary\n",
    "    processed_emails = {}\n",
    "    unmatched_count = 1\n",
    "\n",
    "    for names, emails in author_emails.items():\n",
    "        name_list = [name.strip() for name in names.split(',')]\n",
    "        email_list = [email.strip() for email in emails.split(',')]\n",
    "\n",
    "        # Create a map for name substrings\n",
    "        name_map = {name.split()[-1].lower(): name for name in name_list}\n",
    "\n",
    "        for email in email_list:\n",
    "            email_user = email.split('@')[0].lower()\n",
    "            matched = False\n",
    "\n",
    "            for key, full_name in name_map.items():\n",
    "                if key in email_user:\n",
    "                    processed_emails[full_name] = email\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "            if not matched:\n",
    "                processed_emails[f'unmatched_name_{unmatched_count}'] = email\n",
    "                unmatched_count += 1\n",
    "\n",
    "        # Assign empty string to names without an email address\n",
    "        for name in name_list:\n",
    "            if name not in processed_emails:\n",
    "                processed_emails[name] = ''\n",
    "\n",
    "    return processed_emails\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove unwanted LaTeX commands\n",
    "    text = re.sub(r'\\\\[A-Za-z]+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Normalize whitespace\n",
    "    text = text.replace('†', '')  # Remove any specific unwanted characters\n",
    "    text = text.replace('&', '')  # Remove leading & symbols\n",
    "    return text\n",
    "\n",
    "def extract_emails_ltx_contact(url, sec_to_timeout=10):\n",
    "    '''Extract author emails and institutions from an ArXiv paper page'''\n",
    "    # Initialize the WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run headless browser\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Load the page\n",
    "    driver.get(url)\n",
    "\n",
    "    author_emails = {}\n",
    "    author_institutions = {}\n",
    "    unknown_author_counter = 1\n",
    "    \n",
    "    try:\n",
    "        # Wait until the ltx_authors element is present\n",
    "        WebDriverWait(driver, sec_to_timeout).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"ltx_authors\"))\n",
    "        )\n",
    "\n",
    "        authors = driver.find_elements(By.CLASS_NAME, \"ltx_role_author\")\n",
    "\n",
    "        for author in authors:\n",
    "            # Extracting the author's name\n",
    "            name_elem = author.find_element(By.CLASS_NAME, \"ltx_personname\")\n",
    "            name = clean_text(name_elem.text.strip())\n",
    "\n",
    "            # Extracting the email address\n",
    "            email_elem = author.find_elements(By.CLASS_NAME, \"ltx_contact.ltx_role_email\")\n",
    "            email = email_elem[0].text.strip() if email_elem else ''\n",
    "\n",
    "            # Cleaning the name if it has unwanted characters\n",
    "            name = re.sub(r'[*\\d]', '', name).strip()\n",
    "\n",
    "            # Handling cases with multiple authors in one element\n",
    "            if ' and ' in name or ', ' in name:\n",
    "                multiple_names = re.split(r' and |, ', name)\n",
    "                for n in multiple_names:\n",
    "                    n = clean_text(n)\n",
    "                    if not n:\n",
    "                        n = f'unknown_author_{unknown_author_counter}'\n",
    "                        unknown_author_counter += 1\n",
    "                    author_emails[n.strip()] = email\n",
    "            else:\n",
    "                name = clean_text(name)\n",
    "                if not name:\n",
    "                    name = f'unknown_author_{unknown_author_counter}'\n",
    "                    unknown_author_counter += 1\n",
    "                author_emails[name] = email\n",
    "\n",
    "            # Extracting the institution\n",
    "            institution_elems = author.find_elements(By.CLASS_NAME, \"ltx_contact.ltx_role_affiliation\")\n",
    "            if institution_elems:\n",
    "                for inst_elem in institution_elems:\n",
    "                    institution = clean_text(inst_elem.text.strip())\n",
    "                    if institution not in author_institutions:\n",
    "                        author_institutions[institution] = []\n",
    "                    author_institutions[institution].append(name)\n",
    "\n",
    "        for institution in author_institutions:\n",
    "            author_institutions[institution] = list(set(author_institutions[institution]))\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return {'emails': author_emails, 'institutions': author_institutions}\n",
    "    \n",
    "# debug\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6bd9b97-0cef-4ca8-b35e-3de665bf1b2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n#0 0x561d037d971a <unknown>\n#1 0x561d034aa640 <unknown>\n#2 0x561d034f9c0b <unknown>\n#3 0x561d034f9ef1 <unknown>\n#4 0x561d0353db64 <unknown>\n#5 0x561d0351c90d <unknown>\n#6 0x561d0353b08a <unknown>\n#7 0x561d0351c683 <unknown>\n#8 0x561d034ecd71 <unknown>\n#9 0x561d034ed7de <unknown>\n#10 0x561d037a12ab <unknown>\n#11 0x561d037a5242 <unknown>\n#12 0x561d0378e665 <unknown>\n#13 0x561d037a5dd2 <unknown>\n#14 0x561d037732af <unknown>\n#15 0x561d037c8eb8 <unknown>\n#16 0x561d037c9090 <unknown>\n#17 0x561d037d84ec <unknown>\n#18 0x1550799d16ea start_thread\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 57\u001b[0m\n\u001b[1;32m      2\u001b[0m email_url_list \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://arxiv.org/html/2408.00630v1\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m4\u001b[39m, \u001b[38;5;66;03m# 4, easy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://arxiv.org/html/2408.00164v1\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m1\u001b[39m, \u001b[38;5;66;03m# X 1 trivial, 1 mail\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://arxiv.org/html/2407.21724v1\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m2\u001b[39m, \u001b[38;5;66;03m# tricky, linked all over.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m }\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m url, trg_number \u001b[38;5;129;01min\u001b[39;00m email_url_list\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mextract_emails_ltx_contact\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mprint\u001b[39m()\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 268\u001b[0m, in \u001b[0;36mextract_emails_ltx_contact\u001b[0;34m(url, sec_to_timeout)\u001b[0m\n\u001b[1;32m    264\u001b[0m unknown_author_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;66;03m# Wait until the ltx_authors element is present\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m     \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msec_to_timeout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mltx_authors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     authors \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mltx_role_author\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m author \u001b[38;5;129;01min\u001b[39;00m authors:\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# Extracting the author's name\u001b[39;00m\n",
      "File \u001b[0;32m/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/selenium/webdriver/support/wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n#0 0x561d037d971a <unknown>\n#1 0x561d034aa640 <unknown>\n#2 0x561d034f9c0b <unknown>\n#3 0x561d034f9ef1 <unknown>\n#4 0x561d0353db64 <unknown>\n#5 0x561d0351c90d <unknown>\n#6 0x561d0353b08a <unknown>\n#7 0x561d0351c683 <unknown>\n#8 0x561d034ecd71 <unknown>\n#9 0x561d034ed7de <unknown>\n#10 0x561d037a12ab <unknown>\n#11 0x561d037a5242 <unknown>\n#12 0x561d0378e665 <unknown>\n#13 0x561d037a5dd2 <unknown>\n#14 0x561d037732af <unknown>\n#15 0x561d037c8eb8 <unknown>\n#16 0x561d037c9090 <unknown>\n#17 0x561d037d84ec <unknown>\n#18 0x1550799d16ea start_thread\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "email_url_list = {\n",
    "    'https://arxiv.org/html/2408.00630v1' : 4, # 4, easy\n",
    "    'https://arxiv.org/html/2408.00164v1' : 1, # X 1 trivial, 1 mail\n",
    "    'https://arxiv.org/html/2408.00685v1' : 1, # X 1, easy\n",
    "    'https://arxiv.org/html/2408.00615v1' : 1, # 1, easy\n",
    "    'https://arxiv.org/html/2405.01212v1' : 1, # 1  easy: 1 mail at btm\n",
    "    'https://arxiv.org/html/2405.01201v1' : 1, # 1  easy: again\n",
    "    'https://arxiv.org/html/2405.01380v1' : 1, # 1 easy\n",
    "    'https://arxiv.org/html/2407.18948v1' : 1, # 1 easy, 1 email\n",
    "    'https://arxiv.org/html/2407.20418v1' : 3, # 3, easy\n",
    "    'https://arxiv.org/html/2405.01297v1' : 5, # 5 emails individually\n",
    "    'https://arxiv.org/html/2405.01348v1' : 2, # 2 mails\n",
    "    'https://arxiv.org/html/2405.01407v2' : 1, # 1 email, followed by institute (deloitte)\n",
    "    'https://arxiv.org/html/2408.00095v1' : 2, # 2 emails, bit messy\n",
    "    'https://arxiv.org/html/2408.00013v1' : 4, # 4 mails\n",
    "    'https://arxiv.org/html/2407.21070v1' : 1, # 1 email\n",
    "    'https://arxiv.org/html/2407.21132v1' : 2, # 2 emails\n",
    "    'https://arxiv.org/html/2407.20457v1' : 1, # 1 tricky: email hidden at the end\n",
    "    'https://arxiv.org/html/2405.00661v1' : 2, # 2, leading $1$ that requires filtering\n",
    "    'https://arxiv.org/html/2408.00014v1' : 3, # 3 emails\n",
    "    'https://arxiv.org/html/2408.00024v1' : 2, # 2 tricky, 2 mails at the end of names\n",
    "    'https://arxiv.org/html/2408.00041v1' : 7, # 7 emails right after each name\n",
    "    'https://arxiv.org/html/2408.00014v1' : 3, # 3 mails at btm\n",
    "    'https://arxiv.org/html/2408.00003v1' : 3, # 3 mails at btm followed by `email\"` each\n",
    "    'https://arxiv.org/html/2408.00265v1' : 4, # 4 emails below name\n",
    "    'https://arxiv.org/html/2408.00131v1' : 1, # 1 challenging: 1 mail hyperlinked to one of the names\n",
    "    'https://arxiv.org/html/2402.06758v3' : 2, # 2 challenging 2 mails hyperlinked\n",
    "    'https://arxiv.org/html/2408.00291v1' : 2, # 2 mails at the end\n",
    "    'https://arxiv.org/html/2408.00732v1' : 1, # 1 email at the end\n",
    "    'https://arxiv.org/html/2408.00688v1' : 1, # 1 tricky, 1 email at the bottom\n",
    "    'https://arxiv.org/html/2408.00721v1' : 4, # 4, at the end of names\n",
    "    'https://arxiv.org/html/2408.00670v1' : 3, # 3 challenging, burried in names\n",
    "    'https://arxiv.org/html/2408.00336v1' : 2, # 2, tricky, only 2 authors\n",
    "    'https://arxiv.org/html/2407.20927v1' : 1, # 1, tricky, only one author\n",
    "    'https://arxiv.org/html/2408.00322v1' : 1, # 1, tricky, not first but second though\n",
    "    'https://arxiv.org/html/2407.20747v1' : 2, # 2, tricky,\n",
    "    'https://arxiv.org/html/2408.00757v1' : 2, # 2 tricky\n",
    "    'https://arxiv.org/html/2408.00725v1' : 4, # 4 tricky, linked with name\n",
    "    'https://arxiv.org/html/2408.00709v1' : 0, # challenging, links with names but NO email adresses\n",
    "    'https://arxiv.org/html/2408.00666v1' : 5, # tricky, linkes w name\n",
    "    'https://arxiv.org/html/2408.00614v1' : 3, # linked with name\n",
    "    'https://arxiv.org/html/2407.18727v1' : 1, # one, linked\n",
    "    'https://arxiv.org/html/2407.19187v1' : 2, # tricky, layout appears kaputt\n",
    "    'https://arxiv.org/html/2407.20053v1' : 7, # linked\n",
    "    'https://arxiv.org/html/2407.19909v1' : 4, # linked\n",
    "    'https://arxiv.org/html/2407.20624v1' : 2, # \n",
    "    'https://arxiv.org/html/2407.18638v2' : 2, # 2 emails but 16 authors\n",
    "    'https://arxiv.org/html/2408.00509v1' : 1, # tricky, 1 asterisk with 3 authors\n",
    "    'https://arxiv.org/html/2408.00560v1' : 2, # tricky, 2 mails but a dozen authors or so\n",
    "    'https://arxiv.org/html/2408.00589v1' : 1, # tricky, 1 email but 3 authors, linked at end\n",
    "    'https://arxiv.org/html/2408.00111v1' : 2, # challenging, 2 emails at the end to at the beginning linked via \"equally controbuted\"\n",
    "    'https://arxiv.org/html/2407.21724v1' : 2, # tricky, linked all over.\n",
    "}\n",
    "\n",
    "for url, trg_number in email_url_list.items():\n",
    "    print(extract_emails_ltx_contact(url))\n",
    "    print()\n",
    "    break\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7022b852-49ee-4904-a53b-3aed5c9d6f37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/html/2408.00630v1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26c8a00-3897-42f9-bf6b-c0f81ca3a340",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'emails': {'T. Thiemann  Inst. for Quantum Gravity': '', 'FAU Erlangen – Nürnberg': '', 'Staudtstr.': '', 'Erlangen': '', 'Germany': ''}, 'institutions': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d32dae5e-64c9-45be-b9a9-a08340cf07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract author emails from the given URL using Selenium for ltx_contact ltx_role_email\n",
    "\n",
    "# Function to extract author emails and institutions from the given URL using Selenium for ltx_contact ltx_role_email\n",
    "def extract_emails_ltx_contact_1(url, sec_to_timeout=10):\n",
    "    '''Newest\n",
    "    '''\n",
    "    # Initialize the WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run headless browser\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Load the page\n",
    "    driver.get(url)\n",
    "\n",
    "    author_emails = {}\n",
    "    author_institutions = {}\n",
    "\n",
    "    try:\n",
    "        # Wait for the authors and emails to be present\n",
    "        authors_div = WebDriverWait(driver, sec_to_timeout).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"ltx_authors\"))\n",
    "        )\n",
    "\n",
    "        # Extract author names and superscripts\n",
    "        author_elements = authors_div.find_elements(By.CLASS_NAME, \"ltx_personname\")\n",
    "        authors = [author.text.strip() for author in author_elements]\n",
    "\n",
    "        # Extract institutions superscripts\n",
    "        institution_superscripts = []\n",
    "        for author in authors:\n",
    "            superscripts = re.findall(r'\\d+', author)\n",
    "            institution_superscripts.append(superscripts)\n",
    "\n",
    "        # Remove superscripts from author names\n",
    "        author_names = []\n",
    "        for author in authors:\n",
    "            clean_name = re.sub(r'\\d+', '', author).strip()\n",
    "            author_names.append(clean_name)\n",
    "\n",
    "        # Extract emails\n",
    "        email_elements = driver.find_elements(By.CLASS_NAME, \"ltx_contact.ltx_role_email\")\n",
    "        emails = [email.text.strip() for email in email_elements]\n",
    "\n",
    "        # Create a dictionary with author names and emails\n",
    "        author_emails = {author: email for author, email in zip(author_names, emails)}\n",
    "\n",
    "        # Extract institutions\n",
    "        institution_elements = driver.find_elements(By.CLASS_NAME, \"ltx_contact.ltx_role_address\")\n",
    "        institution_texts = [inst.text.strip() for inst in institution_elements]\n",
    "\n",
    "        # Handle cases where institution and email are directly under author\n",
    "        author_notes_divs = authors_div.find_elements(By.CLASS_NAME, \"ltx_author_notes\")\n",
    "        for note in author_notes_divs:\n",
    "            try:\n",
    "                author_name_element = note.find_element(By.XPATH, './preceding-sibling::span[@class=\"ltx_personname\"]')\n",
    "                author_name = re.sub(r'\\d+', '', author_name_element.text).strip()\n",
    "                email_elements = note.find_elements(By.CLASS_NAME, \"ltx_contact ltx_role_email\")\n",
    "                address_elements = note.find_elements(By.CLASS_NAME, \"ltx_contact ltx_role_address\")\n",
    "                for email_element in email_elements:\n",
    "                    email = email_element.text.strip()\n",
    "                    author_emails[author_name] = email\n",
    "                for address_element in address_elements:\n",
    "                    institution = address_element.text.strip()\n",
    "                    institution = re.sub(r'^\\d+', '', institution).strip()  # Remove leading digits\n",
    "                    if institution not in author_institutions:\n",
    "                        author_institutions[institution] = []\n",
    "                    author_institutions[institution].append(author_name)\n",
    "            except NoSuchElementException:\n",
    "                continue  # Skip if no preceding sibling with class \"ltx_personname\"\n",
    "\n",
    "        # Create dictionary for author institutions\n",
    "        for author, superscripts in zip(author_names, institution_superscripts):\n",
    "            for superscript in superscripts:\n",
    "                if superscript.isdigit():\n",
    "                    index = int(superscript) - 1\n",
    "                    if index < len(institution_texts):\n",
    "                        institution = institution_texts[index]\n",
    "                        institution = re.sub(r'^\\d+', '', institution).strip()  # Remove leading digits\n",
    "                        if institution not in author_institutions:\n",
    "                            author_institutions[institution] = []\n",
    "                        author_institutions[institution].append(author)\n",
    "\n",
    "        # Additional case handling for when author, institution, and email are in a different structure\n",
    "        text_elements = driver.find_elements(By.CLASS_NAME, \"ltx_text.ltx_font_sansserif\")\n",
    "        for element in text_elements:\n",
    "            text = element.text.strip()\n",
    "            if re.search(r'@', text):  # This is an email address, not an institution\n",
    "                author_match = re.search(r'^(.*?)(?=[\\s]*@)', text)\n",
    "                if author_match:\n",
    "                    author_name = author_match.group(1).strip()\n",
    "                    author_name = re.sub(r'\\d+', '', author_name).strip()\n",
    "                    email = text.strip()\n",
    "                    author_emails[author_name] = email\n",
    "                continue  # Skip further processing for email elements\n",
    "            \n",
    "            # Process for institution elements\n",
    "            if re.search(r'Inst\\.|Department|Univ\\.|School|Affiliation', text):\n",
    "                institution = text.strip()\n",
    "                institution = re.sub(r'^\\d+', '', institution).strip()  # Remove leading digits\n",
    "                # Extract the author name related to this institution\n",
    "                preceding_author_elements = element.find_elements(By.XPATH, 'preceding::span[@class=\"ltx_personname\"]')\n",
    "                if preceding_author_elements:\n",
    "                    author_name = re.sub(r'\\d+', '', preceding_author_elements[-1].text).strip()\n",
    "                    if institution not in author_institutions:\n",
    "                        author_institutions[institution] = []\n",
    "                    author_institutions[institution].append(author_name)\n",
    "\n",
    "        # Additional handling for affiliations in ltx_author_notes\n",
    "        affiliation_elements = driver.find_elements(By.CLASS_NAME, \"ltx_contact.ltx_role_affiliation\")\n",
    "        for element in affiliation_elements:\n",
    "            institution = element.text.strip()\n",
    "            institution = re.sub(r'^\\d+', '', institution).strip()  # Remove leading digits\n",
    "            author_name_element = element.find_element(By.XPATH, './preceding::span[@class=\"ltx_personname\"]')\n",
    "            author_name = re.sub(r'\\d+', '', author_name_element.text).strip()\n",
    "            if institution not in author_institutions:\n",
    "                author_institutions[institution] = []\n",
    "            author_institutions[institution].append(author_name)\n",
    "\n",
    "    except TimeoutException:\n",
    "        # Handle timeout exception\n",
    "        author_emails = {}\n",
    "        author_institutions = {}\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "    return {'emails': author_emails, 'institutions': author_institutions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dfc0d8-ee8f-457f-9798-f6a088ce3767",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output = {\n",
    "    'https://arxiv.org/html/2408.00164v1' : {'emails': {'Manish Ramchander': 'manishd@imsc.res.in'}, 'institutions': {}},\n",
    "    'https://arxiv.org/html/2408.00685v1' : {'emails': {'Debmalya Sain': 'saindebmalya@gmail.com'},\n",
    "                                             'institutions': {'(Sain) Department of Mathematics, Indian Institute of Information Technology Raichur, Karnataka 584135, India': ['Debmalya Sain']}},\n",
    "    '' : '',\n",
    "    \n",
    "    \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca0ac878-3b02-45bf-b034-5732208cebac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_expected_outputs = [\n",
    "{'emails': {'Giulia Cusin' : 'cusin@iap.fr', 'Cyril Pitrou' : 'pitrou@iap.fr', 'Camille Bonvin' : 'camille.bonvin@unige.ch', \n",
    "            'Aurélien Barrau' : 'barrau@lpsc.in2p3.fr'},\n",
    " 'institutions': {'Institut d’Astrophysique de Paris, UMR-7095 du CNRS et de Sorbonne Université, Paris, France' : ['Giulia Cusin', 'Cyril Pitrou'],\n",
    "                  'Département de Physique Théorique and Center for Astroparticle Physics, Université de Genève, Quai E. Ansermet 24, CH-1211 Genève 4, Switzerland' : ['Giulia Cusin','Camille Bonvin'],\n",
    "                  'Laboratoire de Physique Subatomique et de Cosmologie, Univ. Grenoble-Alpes, CNRS-IN2P3, 53 av. des Martyrs, 38026 Grenoble, France' : ['Aurélien Barrau', 'Killian Martineau']}}\n",
    "\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0847cd1b-5fdd-46d4-9f99-603e4fa8beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ten_examples_provided = [\n",
    "    'https://arxiv.org/html/2405.01297v1',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "50be5ea7-475d-40a8-ab2c-fc236655b67a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://arxiv.org/html/2408.00164v1'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bfcbe966-bd60-435d-a610-9a676c8b39a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'emails': {'Debmalya Sain': 'saindebmalya@gmail.com'}, 'institutions': {}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_emails_ltx_contact_5(keys[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085178b7-a995-484a-a63d-22d99b669b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_emails['institutions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b116a-3120-40c9-86c5-943088615ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = ['https://arxiv.org/html/2408.00175v1',\n",
    "           'https://arxiv.org/html/2408.00524v1',\n",
    "           'https://arxiv.org/html/2406.00051v1',\n",
    "           'https://arxiv.org/html/2408.00168v1',\n",
    "           'https://arxiv.org/html/2408.00722v1',\n",
    "           'https://arxiv.org/html/2408.00571v1',\n",
    "           'https://arxiv.org/html/2407.18402v1',\n",
    "           'https://arxiv.org/html/2407.18820v1',\n",
    "           'https://arxiv.org/html/2407.18426v1',\n",
    "           'https://arxiv.org/html/2405.00116v1',\n",
    "           'https://arxiv.org/html/2405.00374v1',\n",
    "           'https://arxiv.org/html/2405.00661v1']\n",
    "\n",
    "for url_loc in url_list:\n",
    "    # be kind\n",
    "    time.sleep(0.3)\n",
    "    # print\n",
    "    print(post_process_emails(extract_author_emails(url_loc)))\n",
    "    #print(extract_abstract(url_loc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383671a6-f07d-440b-91a8-f90aab761470",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_text = download_plain_text_from_html('https://arxiv.org/html/2402.14703')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1640447-c565-49b7-a6e8-b74033ae8923",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_emails(author_emails:dict):\n",
    "    \"\"\"\n",
    "    Post-processes author/email dictinary extracted from ArXiV to match author names and email addresses (if present) \n",
    "    \"\"\"\n",
    "    # Initialize result dictionary\n",
    "    processed_emails = {}\n",
    "    unmatched_count = 1\n",
    "\n",
    "    for names, emails in author_emails.items():\n",
    "        name_list = [name.strip() for name in names.split(',')]\n",
    "        email_list = [email.strip() for email in emails.split(',')]\n",
    "\n",
    "        # Create a map for name substrings\n",
    "        name_map = {name.split()[-1].lower(): name for name in name_list}\n",
    "\n",
    "        for email in email_list:\n",
    "            email_user = email.split('@')[0].lower()\n",
    "            matched = False\n",
    "\n",
    "            for key, full_name in name_map.items():\n",
    "                if key in email_user:\n",
    "                    processed_emails[full_name] = email\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "            if not matched:\n",
    "                processed_emails[f'unmatched_name_{unmatched_count}'] = email\n",
    "                unmatched_count += 1\n",
    "\n",
    "        # Assign empty string to names without an email address\n",
    "        for name in name_list:\n",
    "            if name not in processed_emails:\n",
    "                processed_emails[name] = ''\n",
    "\n",
    "    return processed_emails\n",
    "\n",
    "# Example usage\n",
    "author_emails = {\n",
    "    'Isabella Danhoni, Guy D. Moore': 'idanhoni@theorie.ikp.physik.tu-darmstadt.de,guy.moore@physik.tu-darmstadt.de'\n",
    "}\n",
    "processed_emails = post_process_emails(author_emails)\n",
    "print(\"Processed Emails:\", processed_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d8a17b-d068-40fa-af6a-9961458cbbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send a GET request to the URL\n",
    "url = 'https://arxiv.org/html/2402.14703'\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()  # Check if the request was successful\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the div containing the date and domain\n",
    "watermark_div = soup.select_one('div#watermark-tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b280d1d-5322-486c-b703-d944d047ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a981b3-b279-4eb6-8866-d86a40bb40aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo",
   "language": "python",
   "name": "bo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
