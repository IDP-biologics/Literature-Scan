{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47d0aa95-a554-4a37-930d-41a0764f037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo ArXiV\n",
    "import arxiv\n",
    "import re\n",
    "import time\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93825b3f-1f47-4e4a-8883-16c272a2fefd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd874af3-b2bf-4ebc-8020-ff651a2eb8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Search ArXiV for ≤1 papers for each of the 1 search words of 0 categories...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Search words...: 100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
     ]
    },
    {
     "ename": "TimeoutException",
     "evalue": "Message: \nStacktrace:\n#0 0x559da3c5f71a <unknown>\n#1 0x559da3930640 <unknown>\n#2 0x559da397fc0b <unknown>\n#3 0x559da397fef1 <unknown>\n#4 0x559da39c3b64 <unknown>\n#5 0x559da39a290d <unknown>\n#6 0x559da39c108a <unknown>\n#7 0x559da39a2683 <unknown>\n#8 0x559da3972d71 <unknown>\n#9 0x559da39737de <unknown>\n#10 0x559da3c272ab <unknown>\n#11 0x559da3c2b242 <unknown>\n#12 0x559da3c14665 <unknown>\n#13 0x559da3c2bdd2 <unknown>\n#14 0x559da3bf92af <unknown>\n#15 0x559da3c4eeb8 <unknown>\n#16 0x559da3c4f090 <unknown>\n#17 0x559da3c5e4ec <unknown>\n#18 0x14e537f1f6ea start_thread\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutException\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m arx\u001b[38;5;241m.\u001b[39mfind_papers()\n\u001b[1;32m      7\u001b[0m arx\u001b[38;5;241m.\u001b[39mget_urls(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHouse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m abstr \u001b[38;5;241m=\u001b[39m \u001b[43marx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_emails_ltx_contact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp://arxiv.org/html/2408.01423v1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m abstr\n",
      "Cell \u001b[0;32mIn[7], line 287\u001b[0m, in \u001b[0;36mNEW_ArXiV_HTML_Parser.extract_emails_ltx_contact\u001b[0;34m(self, url, sec_to_timeout)\u001b[0m\n\u001b[1;32m    283\u001b[0m unknown_author_counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Wait until the ltx_authors element is present\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msec_to_timeout\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mltx_authors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m     authors \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mCLASS_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mltx_role_author\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m author \u001b[38;5;129;01min\u001b[39;00m authors:\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;66;03m# Extracting the author's name\u001b[39;00m\n",
      "File \u001b[0;32m/eagle/projects/tpc/siebenschuh/envs_/bo/lib/python3.11/site-packages/selenium/webdriver/support/wait.py:105\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m TimeoutException(message, screen, stacktrace)\n",
      "\u001b[0;31mTimeoutException\u001b[0m: Message: \nStacktrace:\n#0 0x559da3c5f71a <unknown>\n#1 0x559da3930640 <unknown>\n#2 0x559da397fc0b <unknown>\n#3 0x559da397fef1 <unknown>\n#4 0x559da39c3b64 <unknown>\n#5 0x559da39a290d <unknown>\n#6 0x559da39c108a <unknown>\n#7 0x559da39a2683 <unknown>\n#8 0x559da3972d71 <unknown>\n#9 0x559da39737de <unknown>\n#10 0x559da3c272ab <unknown>\n#11 0x559da3c2b242 <unknown>\n#12 0x559da3c14665 <unknown>\n#13 0x559da3c2bdd2 <unknown>\n#14 0x559da3bf92af <unknown>\n#15 0x559da3c4eeb8 <unknown>\n#16 0x559da3c4f090 <unknown>\n#17 0x559da3c5e4ec <unknown>\n#18 0x14e537f1f6ea start_thread\n"
     ]
    }
   ],
   "source": [
    "arx = NEW_ArXiV_HTML_Parser()\n",
    "arx.search_words = ['LLM']\n",
    "arx.max_results = 1\n",
    "\n",
    "arx.find_papers()\n",
    "\n",
    "arx.get_urls(\"House\", 2)\n",
    "\n",
    "abstr = arx.extract_emails_ltx_contact('http://arxiv.org/html/2408.01423v1')\n",
    "abstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bea1f07-4ee8-4c65-9932-4cbcfa466257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_abstract(url, sec_to_timeout=10):\n",
    "    # Initialize the WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')  # Run headless browser\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    # Load the page\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait for the abstract div to be present\n",
    "        abstract_div = WebDriverWait(driver, sec_to_timeout).until(\n",
    "            EC.presence_of_element_located((By.ID, \"abstract\"))\n",
    "        )\n",
    "\n",
    "        # Extract the text from the div\n",
    "        abstract_text = abstract_div.text.strip()\n",
    "    except TimeoutException:\n",
    "        # Handle timeout exception\n",
    "        abstract_text = None\n",
    "    finally:\n",
    "        # Close the WebDriver\n",
    "        driver.quit()\n",
    "\n",
    "    return abstract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e6e670d6-ca3a-4694-96ee-c922a3c3a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arx.load_page_content('http://arxiv.org/html/2408.01423v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ccf5b7c5-bed4-489d-b1f0-b980d1124028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'http://arxiv.org/html/2408.01423v1': '<html><head><meta http-equiv=\"origin-trial\" content=\"A/kargTFyk8MR5ueravczef/wIlTkbVk1qXQesp39nV+xNECPdLBVeYffxrM8TmZT6RArWGQVCJ0LRivD7glcAUAAACQeyJvcmlnaW4iOiJodHRwczovL2dvb2dsZS5jb206NDQzIiwiZmVhdHVyZSI6IkRpc2FibGVUaGlyZFBhcnR5U3RvcmFnZVBhcnRpdGlvbmluZzIiLCJleHBpcnkiOjE3NDIzNDIzOTksImlzU3ViZG9tYWluIjp0cnVlLCJpc1RoaXJkUGFydHkiOnRydWV9\">\\n       <title>arXiv reCAPTCHA</title>\\n       <link rel=\"stylesheet\" type=\"text/css\" media=\"screen\" href=\"https://static.arxiv.org/static/browse/0.3.2.8/css/arXiv.css?v=20220215\">\\n       <script type=\"text/javascript\" async=\"\" charset=\"utf-8\" src=\"https://www.gstatic.com/recaptcha/releases/hfUfsXWZFeg83qqxrK27GB8P/recaptcha__en.js\" crossorigin=\"anonymous\" integrity=\"sha384-CUWl3AwpNax4J2/ffIDxMr+sLnEFg8pkj5QqZtMtJZtrL5bS5QTkIoza4qZakWjL\"></script><script src=\"https://www.google.com/recaptcha/api.js\" async=\"\" defer=\"\"></script>\\n       <script>\\n         var submitForm = function () {\\n             document.forms[\\'rrr\\'].submit();\\n         }\\n       </script>\\n     </head>\\n\\n     <body class=\"with-cu-identity\">\\n\\n       <div id=\"cu-identity\">\\n         <div id=\"cu-logo\">\\n           <a href=\"https://www.cornell.edu/\"><img src=\"https://static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg\" alt=\"Cornell University\" width=\"200\" border=\"0\"></a>\\n         </div>\\n         <div id=\"support-ack\">\\n           <a href=\"https://confluence.cornell.edu/x/ALlRF\">We gratefully acknowledge support from<br>\\n             the Simons Foundation and member institutions.</a>\\n         </div>\\n       </div>\\n       <div id=\"header\">\\n         <h1 class=\"header-breadcrumbs\"><a href=\"/\">\\n             <img src=\"https://static.arxiv.org/images/arxiv-logo-one-color-white.svg\" aria-label=\"logo\" alt=\"arxiv logo\" width=\"85\" style=\"width:85px;margin-right:8px;\"></a></h1>\\n       </div>\\n\\n       <form id=\"rrr\" action=\"\" method=\"GET\">\\n         <div class=\"g-recaptcha\" data-sitekey=\"6Lcs9MMpAAAAAIbNRdl5t5naTQeb9ZruBQ8dkXW0\" data-callback=\"submitForm\"><div style=\"width: 304px; height: 78px;\"><div><iframe title=\"reCAPTCHA\" width=\"304\" height=\"78\" role=\"presentation\" name=\"a-d9p5t8xjo69\" frameborder=\"0\" scrolling=\"no\" sandbox=\"allow-forms allow-popups allow-same-origin allow-scripts allow-top-navigation allow-modals allow-popups-to-escape-sandbox allow-storage-access-by-user-activation\" src=\"https://www.google.com/recaptcha/api2/anchor?ar=1&amp;k=6Lcs9MMpAAAAAIbNRdl5t5naTQeb9ZruBQ8dkXW0&amp;co=aHR0cDovL2FyeGl2Lm9yZzo4MA..&amp;hl=en&amp;v=hfUfsXWZFeg83qqxrK27GB8P&amp;size=normal&amp;cb=wik1629rppu8\"></iframe></div><textarea id=\"g-recaptcha-response\" name=\"g-recaptcha-response\" class=\"g-recaptcha-response\" style=\"width: 250px; height: 40px; border: 1px solid rgb(193, 193, 193); margin: 10px 25px; padding: 0px; resize: none; display: none;\"></textarea></div><iframe style=\"display: none;\"></iframe></div>\\n         <br>\\n         <input type=\"submit\" value=\"Submit\">\\n       </form>\\n\\n       <footer style=\"clear: both;\">\\n         <div class=\"\">\\n           <ul style=\"list-style: none; line-height: 2;\">\\n             <li><a href=\"https://arxiv.org/help/web_accessibility\">Web Accessibility Assistance</a></li>\\n           </ul>\\n         </div>\\n       </footer>\\n        \\n      \\n    </body></html>'}\n"
     ]
    }
   ],
   "source": [
    "print(arx.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1ed4852-f9a9-4e21-8c0f-dc94d50a754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NEW_ArXiV_HTML_Parser:\n",
    "    def __init__(self, wait_time:float=0.1, \n",
    "                 max_results:int=200, \n",
    "                 categories=None, \n",
    "                 keywords_file_path:Path=Path('./config/search_words.yaml')):\n",
    "        self.urls = []\n",
    "        self.wait_time = wait_time\n",
    "        self.driver = None\n",
    "        self.page_content = {}\n",
    "        self.keywords_file_path = Path(keywords_file_path)\n",
    "        self.max_results = max_results\n",
    "        self.categories = categories\n",
    "\n",
    "        # load search words\n",
    "        self.load_keywords_from_categories(categories=self.categories, file_path=self.keywords_file_path)\n",
    "        \n",
    "    def init_webdriver(self):\n",
    "        '''\n",
    "        Works\n",
    "        '''\n",
    "        if not self.driver:\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument('--headless')\n",
    "            self.driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    def load_keywords_from_categories(self, categories=None, file_path='./config/search_words.yaml'):\n",
    "        '''\n",
    "        Works\n",
    "        Load keywords from config YAML according to categories\n",
    "        '''\n",
    "        assert self.keywords_file_path.is_file(), \"File doesn`t exist contaiing keywords.\"\n",
    "\n",
    "        # load\n",
    "        with open(file_path, 'r') as file:\n",
    "            keywords_dict = yaml.safe_load(file)\n",
    "    \n",
    "        # categories\n",
    "        if categories is None or len(categories)==0:\n",
    "            categories = [\n",
    "                \"Physics\",\n",
    "                \"Computer Science\",\n",
    "                \"Mathematics\",\n",
    "                \"Quantitative Biology\",\n",
    "                \"Quantitative Finance\",\n",
    "                \"Statistics\",\n",
    "                \"Interdisciplinary\"\n",
    "            ]\n",
    "    \n",
    "        # filter keywords\n",
    "        selected_keywords = []\n",
    "        for category in categories:\n",
    "            if category in keywords_dict:\n",
    "                selected_keywords.extend(keywords_dict[category])\n",
    "        \n",
    "        self.search_words = selected_keywords\n",
    "\n",
    "    def get_arxiv_articles_with_html(self, query, max_results=100):\n",
    "        \"\"\"\n",
    "        Works\n",
    "        \"\"\"\n",
    "        client = arxiv.Client()\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "        )\n",
    "        \n",
    "        articles_with_html = []\n",
    "        \n",
    "        for result in client.results(search):\n",
    "            if any(link.title == \"pdf\" for link in result.links):\n",
    "                # check if HTML-compatible\n",
    "                html_url = result.entry_id.replace('/abs/', '/html/')\n",
    "                time.sleep(self.wait_time * 0.1)\n",
    "                response = requests.get(html_url)\n",
    "                # load\n",
    "                if response.status_code == 200:\n",
    "                    articles_with_html.append({\n",
    "                        'id': result.entry_id,\n",
    "                        'title': result.title,\n",
    "                        'pdf_url': result.entry_id.replace('abs', 'pdf') + '.pdf',\n",
    "                        'html_url': result.entry_id.replace('/abs/', '/html/'),\n",
    "                        'authors' : [aut.name for aut in result.authors],\n",
    "                        'summary' : result.summary,\n",
    "                        'date_published' : result.published,\n",
    "                        'date_updated' : result.updated,\n",
    "                        'doi' : result.doi,\n",
    "                        'prim_cat' : result.primary_category,\n",
    "                        'categories' : result.journal_ref,\n",
    "                        'journal_ref' : result.journal_ref,\n",
    "                    })\n",
    "        \n",
    "        return articles_with_html\n",
    "\n",
    "    def __len__(self,):\n",
    "        \"\"\"\n",
    "        Works\n",
    "        \"\"\"\n",
    "        return len(self.urls)\n",
    "\n",
    "    def find_papers(self,):\n",
    "        \"\"\"\n",
    "        Works\n",
    "        Search ArXiV for papers according to search word list\n",
    "        \"\"\"\n",
    "        if self.categories is None:\n",
    "            self.categories = []\n",
    "        print(f'... Search ArXiV for ≤{self.max_results} papers for each of the {len(self.search_words)} search words of {len(self.categories)} categories...')\n",
    "        \n",
    "        for search_word in tqdm(self.search_words, desc=\"Search words...\"):\n",
    "            self.get_urls(search_word)\n",
    "        \n",
    "    def get_urls(self, query, n:int=-1):\n",
    "        \"\"\"\n",
    "        Works\n",
    "        \"\"\"\n",
    "        self.urls += self.get_arxiv_articles_with_html(query, max_results=self.max_results if n<0 else round(n))\n",
    "            \n",
    "\n",
    "    def load_page_content(self, url):\n",
    "        \"\"\"\n",
    "        ???\n",
    "        \"\"\"\n",
    "        self.init_webdriver()\n",
    "        if url not in self.page_content:\n",
    "            self.driver.get(url)\n",
    "            self.page_content[url] = self.driver.page_source\n",
    "\n",
    "    def download_plain_text_from_html(self, url):\n",
    "        self.load_page_content(url)\n",
    "        soup = BeautifulSoup(self.page_content[url], 'html.parser')\n",
    "        text = soup.get_text()\n",
    "        return text\n",
    "\n",
    "    def beautify_text(self, text):\n",
    "        text = text.strip()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        \n",
    "        replacements = {\n",
    "            r'\\\\xa0': ' ',\n",
    "            r'\\\\u2062': '',\n",
    "            r'\\\\n': '\\n',\n",
    "            r'\\\\t': '\\t',\n",
    "        }\n",
    "        \n",
    "        for pattern, replacement in replacements.items():\n",
    "            text = re.sub(pattern, replacement, text)\n",
    "        \n",
    "        text = re.sub(r'\\\\', '', text)\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def extract_title(self, url):\n",
    "        self.load_page_content(url)\n",
    "        soup = BeautifulSoup(self.page_content[url], 'html.parser')\n",
    "        title_element = soup.find('h1', class_='ltx_title ltx_title_document')\n",
    "        if title_element:\n",
    "            return title_element.get_text(strip=True)\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    def extract_authors(self, url):\n",
    "        self.load_page_content(url)\n",
    "        soup = BeautifulSoup(self.page_content[url], 'html.parser')\n",
    "        authors_div = soup.find('div', class_='ltx_authors')\n",
    "        authors = []\n",
    "        if authors_div:\n",
    "            author_spans = authors_div.find_all('span', class_='ltx_personname')\n",
    "            for span in author_spans:\n",
    "                authors.append(span.get_text(strip=True))\n",
    "        return authors\n",
    "\n",
    "    def extract_date_and_domain(self, url, sec_to_timeout=3):\n",
    "        self.load_page_content(url)\n",
    "        soup = BeautifulSoup(self.page_content[url], 'html.parser')\n",
    "        watermark_div = soup.find(id=\"watermark-tr\")\n",
    "        if watermark_div:\n",
    "            text = watermark_div.text.strip()\n",
    "            match = re.search(r'arXiv:\\d{4}\\.\\d{5}(?:v\\d)? \\[(.*?)\\] (\\d{2} \\w{3} \\d{4})', text)\n",
    "            if match:\n",
    "                domain = match.group(1)\n",
    "                date = match.group(2)\n",
    "            else:\n",
    "                domain = ''\n",
    "                date = ''\n",
    "        else:\n",
    "            domain = ''\n",
    "            date = ''\n",
    "        return {'date': date, 'domain': domain}\n",
    "\n",
    "    def extract_abstract(url, sec_to_timeout=10):\n",
    "        # Initialize the WebDriver\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')  # Run headless browser\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "        # Load the page\n",
    "        driver.get(url)\n",
    "    \n",
    "        try:\n",
    "            # Wait for the abstract div to be present\n",
    "            abstract_div = WebDriverWait(driver, sec_to_timeout).until(\n",
    "                EC.presence_of_element_located((By.ID, \"abstract\"))\n",
    "            )\n",
    "    \n",
    "            # Extract the text from the div\n",
    "            abstract_text = abstract_div.text.strip()\n",
    "        except TimeoutException:\n",
    "            # Handle timeout exception\n",
    "            abstract_text = None\n",
    "        finally:\n",
    "            # Close the WebDriver\n",
    "            driver.quit()\n",
    "    \n",
    "        return abstract_text\n",
    "\n",
    "    def extract_author_emails(self, url, sec_to_timeout=5):\n",
    "        self.load_page_content(url)\n",
    "        soup = BeautifulSoup(self.page_content[url], 'html.parser')\n",
    "        authors_div = soup.find('div', class_='ltx_authors')\n",
    "        \n",
    "        authors = []\n",
    "        if authors_div:\n",
    "            author_elements = authors_div.find_all('span', class_='ltx_personname')\n",
    "            authors = [author.text.strip() for author in author_elements]\n",
    "\n",
    "        email_elements = soup.find_all('span', class_='ltx_contact ltx_role_email')\n",
    "        emails = [email.text.strip() for email in email_elements]\n",
    "\n",
    "        author_emails = {author: email for author, email in zip(authors, emails)}\n",
    "        return author_emails\n",
    "\n",
    "    def post_process_emails(self, author_emails):\n",
    "        processed_emails = {}\n",
    "        unmatched_count = 1\n",
    "\n",
    "        for names, emails in author_emails.items():\n",
    "            name_list = [name.strip() for name in names.split(',')]\n",
    "            email_list = [email.strip() for email in emails.split(',')]\n",
    "\n",
    "            name_map = {name.split()[-1].lower(): name for name in name_list}\n",
    "\n",
    "            for email in email_list:\n",
    "                email_user = email.split('@')[0].lower()\n",
    "                matched = False\n",
    "\n",
    "                for key, full_name in name_map.items():\n",
    "                    if key in email_user:\n",
    "                        processed_emails[full_name] = email\n",
    "                        matched = True\n",
    "                        break\n",
    "\n",
    "                if not matched:\n",
    "                    processed_emails[f'unmatched_name_{unmatched_count}'] = email\n",
    "                    unmatched_count += 1\n",
    "\n",
    "            for name in name_list:\n",
    "                if name not in processed_emails:\n",
    "                    processed_emails[name] = ''\n",
    "\n",
    "        return processed_emails\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'\\\\[A-Za-z]+[*]?', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        text = text.replace('†', '')\n",
    "        text = text.replace('&', '')\n",
    "        return text\n",
    "\n",
    "    def extract_emails_ltx_contact(self, url, sec_to_timeout=10):\n",
    "        '''Extract author emails and institutions from an ArXiv paper page'''\n",
    "        # Initialize the WebDriver\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--headless')  # Run headless browser\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "    \n",
    "        # Load the page\n",
    "        driver.get(url)\n",
    "    \n",
    "        author_emails = {}\n",
    "        author_institutions = {}\n",
    "        unknown_author_counter = 1\n",
    "        \n",
    "        try:\n",
    "            # Wait until the ltx_authors element is present\n",
    "            WebDriverWait(driver, sec_to_timeout).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"ltx_authors\"))\n",
    "            )\n",
    "    \n",
    "            authors = driver.find_elements(By.CLASS_NAME, \"ltx_role_author\")\n",
    "    \n",
    "            for author in authors:\n",
    "                # Extracting the author's name\n",
    "                name_elem = author.find_element(By.CLASS_NAME, \"ltx_personname\")\n",
    "                name = clean_text(name_elem.text.strip())\n",
    "    \n",
    "                # Extracting the email address\n",
    "                email_elem = author.find_elements(By.CLASS_NAME, \"ltx_contact.ltx_role_email\")\n",
    "                email = email_elem[0].text.strip() if email_elem else ''\n",
    "    \n",
    "                # Cleaning the name if it has unwanted characters\n",
    "                name = re.sub(r'[*\\d]', '', name).strip()\n",
    "    \n",
    "                # Handling cases with multiple authors in one element\n",
    "                if ' and ' in name or ', ' in name:\n",
    "                    multiple_names = re.split(r' and |, ', name)\n",
    "                    for n in multiple_names:\n",
    "                        n = clean_text(n)\n",
    "                        if not n:\n",
    "                            n = f'unknown_author_{unknown_author_counter}'\n",
    "                            unknown_author_counter += 1\n",
    "                        author_emails[n.strip()] = email\n",
    "                else:\n",
    "                    name = clean_text(name)\n",
    "                    if not name:\n",
    "                        name = f'unknown_author_{unknown_author_counter}'\n",
    "                        unknown_author_counter += 1\n",
    "                    author_emails[name] = email\n",
    "    \n",
    "                # Extracting the institution\n",
    "                institution_elems = author.find_elements(By.CLASS_NAME, \"ltx_contact.ltx_role_affiliation\")\n",
    "                if institution_elems:\n",
    "                    for inst_elem in institution_elems:\n",
    "                        institution = clean_text(inst_elem.text.strip())\n",
    "                        if institution not in author_institutions:\n",
    "                            author_institutions[institution] = []\n",
    "                        author_institutions[institution].append(name)\n",
    "    \n",
    "            for institution in author_institutions:\n",
    "                author_institutions[institution] = list(set(author_institutions[institution]))\n",
    "    \n",
    "        finally:\n",
    "            driver.quit()\n",
    "    \n",
    "        return {'emails': author_emails, 'institutions': author_institutions}\n",
    "\n",
    "    def extract_all(self, wait_time:float=-1):\n",
    "        if wait_time > 0:\n",
    "            self.wait_time = wait_time\n",
    "        results = []\n",
    "        for article in self.urls:\n",
    "            time.sleep(self.wait_time)\n",
    "            url = article['html_url']\n",
    "            self.load_page_content(url)\n",
    "\n",
    "            # scrape components\n",
    "            title = self.extract_title(url)\n",
    "            date_and_domain = self.extract_date_and_domain(url)\n",
    "            abstract = self.extract_abstract(url)\n",
    "            emails_and_institutions = self.extract_emails_ltx_contact(url)\n",
    "\n",
    "            # scrapped metadata\n",
    "            result = {\n",
    "                'title_scraped': title,\n",
    "                'date_scraped': date_and_domain['date'],\n",
    "                'domain_scraped': date_and_domain['domain'],\n",
    "                'abstract_scraped': abstract,\n",
    "                'emails': emails_and_institutions['emails'],\n",
    "                'institutions': emails_and_institutions['institutions']\n",
    "            }\n",
    "            \n",
    "            # add ArXiV-provided metadata\n",
    "            result.update(article)\n",
    "            \n",
    "            # append\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            self.driver = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0e5603-2db0-4539-a62f-0b4722ce3e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bo",
   "language": "python",
   "name": "bo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
